<div align="center"><img src="readme_pictures\preview.jfif" width="100%" /></div>

<div align="center"><h1>Решение отборочного задания для трека Computer Vision "Детекция логотипа Т-Банка"</h1></div>

## Инструкция по запуску и использованию

Через Docker :whale:
  * **Способ 1 (рекомендуемый)**: использовать готовый предварительно собранный мной образ из DockerHub (в основе лежит образ `ultralytics/ultralytics:latest`)

      **Скачиваем образ:**
      ```
      sudo docker pull annagordova/logo-ultra:latest
      ```

      **Запуск на GPU:**
      ```
      sudo docker run --shm-size 16g --name logo --network host --gpus all annagordova/logo-ultra
      ```

      **Запуск на CPU:**
      ```
      sudo docker run --name logo --network host annagordova/logo-ultra
      ```

  * **Способ 2**: использовать готовый предварительно собранный мной образ из DockerHub (в основе лежит образ `nvidia/cuda:12.8.1-cudnn-devel-ubuntu22.04`) 

      **Скачиваем образ:**
      ```
      sudo docker pull annagordova/logo:latest
      ```

      **Запуск на GPU:**
       ```
      sudo docker run --shm-size 16g --name logo --network host --gpus all annagordova/logo
      ```

      **Запуск на CPU:**
      ```
      sudo docker run --name logo --network host annagordova/logo
      ```

  * **Способ 3**: собрать контейнер из этого репозитория (в основе лежит образ `ultralytics/ultralytics:latest`) 

      **Скачиваем репозиторий:**
      ```
      git clone https://github.com/AnnaGordova/TBank_app.git
      ```

      **Переходим в корень скачанного репозитория:**
      ```
      cd TBank_app
      ```

      **Собираем Docker-образ:**
      ```
      sudo docker build -t logo-detector . 
      ```

      **Запуск на GPU:**
      ```
      sudo docker run --shm-size 16g --name logo --network host --gpus all logo-detector
      ```

      **Запуск на CPU:**
      ```
      sudo docker run --name logo --network host logo-detector
      ```

    REST-API сервис будет запущен по адресу: `http://127.0.0.1:8000`. Swagger документация будет доступна по адресу `http://127.0.0.1:8000/docs`. Эндпоинт `/detect` доступен по адреcу `http://127.0.0.1:8000/detect`. О других доступных эндпоинтах будет рассказано далее. 
  
## Компоненты репозитория

1. `app` - исходный код REST-API сервиса;
2. `data_and_model_preparation` - в этой директории находятся два jupyter notebook с исходным кодом подготовки разметки исходных данных (`TBank_Sirius.ipynb` - первичная разметка, `TBank_Classification.ipynb` - вторичная разметка. Также для удобства прикрепляю ссылки на эти же блокноты в Google Colab: <a href = "https://colab.research.google.com/drive/1zyX4-egit9KCMbJxLIG9cdZHhuQwN8st?usp=sharing"> TBank_Sirius.ipynb </a> и <a href = "https://colab.research.google.com/drive/1HFplOwGUfNeBUre3ZzZtzSp2hBCdTG7b?usp=sharing" > TBank_Classification.ipynb</a>). 
Также в этой директории находится скрипт обучения `YOLOv8` `train_yolo.py`. Обо всех этапах работы с моделями и данными подробно расписано далее в этом README.md файле;
3. :heavy_exclamation_mark: `my_val_dataseet_labeled` - в этой директории находится собранный и размеченный вручную при помощи LabelStudio валидационный датасет (согласно требованию технического задания);
4. `test_model` - в этой директории находятся два python-скрипта валидации на собранном датасеете. 
`test_on_my_val_dataset_labeled.py` позволяет оценить качество модели на этом наборе данных, а `draw_on_my_val_dataset_labeled.py` позволяет отрисовать результаты детекции на изображениях датасета (сохраняет результаты в директорию `draw_my_val_dataset_labeled`);
5. `draw_my_val_dataset_labeled` - директория с отрисованными результатами детекции на валидационной выборке (см. пункт 4 в разделе "Компоненты репозитория");
6. `wheights` - веса модели;
7.  `readme_pictures` - директория с изображениями для README.md файла;
8. `README.md` - инструкция по запуску и отчет о проделанной работе;
9. `Dockerfile` - инструкции для сборки Docker-образа;
9. `requirements.txt` - набор зависимостей для сборки Docker-образа. Так же может быть полезен, если запуск решения будет производиться из исходного кода этого репозитория для того, чтобы проверить список необходимых к установке Python-библиотек.

## Описание подхода к решению 

### :bulb: Анализ поставленной задачи и идеи решения проблем
Основная сложность поставленной задачи заключалась в отсутствии размеченных данных, необходимых для обучения нейронной сети распознаванию и детекции логотипа Т-Банка на изображении. В связи с этим был выбран подход к решению этой проблемы, состоящий из двух частей:
  
  1. разработать метод, который позволит автоматически разметить данные, предоставленные организаторами конкурса, и обучить на них нейронную сеть; 
  2. сгенерировать синтетический набор данных и также обучить на них подобную модель. Затем сравнить метрики точности обеих моделей, чтобы оценить конкурентную способность метода, основанного на автоматической разметке данных.

### Пайплайн системы автоматической разметки и детекции логотипа Т-Банка

Система автоматической разметки представляет собой комбинацию двух моделей:

- `OWLv2` в качестве основы (позволяет решить задачу Zero-Shot Object Detection. Однако, как мы убедимся далее, точность такой разметки мала). 
- Классификатор на основе `resnet18`, дообученный на собранном наборе данных. Цель этой модели - отфильтровать предсказания `OWLv2` и оставить только верные (с найденным логотипом T-Банка).

Полученный набор данных послужил датасетом для обучения модели детекции `YOLOv8`. Подробнее о работе с данными и обучении моделей из описанной системы будет рассказано далее.

### 1-ый этап. Выбор модели OWLV2, эксперименты

`OWLv2` - модель нейронной сети, которая умеет находить и определять границы объектов на картинке (задача object detection). Она использует архитектуру Vision Transformer (ViT). Эксперименты проводились и с другими нейронными сетями, основанными на архитектуре ViT (например, `Grounding DINO`), однако `OWLv2` показала наилучшие результаты. 

`OWLv2` хорошо предсказывает и находит на изображении объекты, которые широко распространены (кошка, дерево, самолет и т.д.). Данная модель не знает, что такое логотип Т-Банка, поэтому проводились эксперименты с различыми текстовыми промптами, его описывающими: "the letter T in the shield", "the shield with the letter T" и т.д. Image-guided подход также не дал хороших результатов. Самые точные предсказания модель выдавала при вводе текстового промпта "The letter T and the shield as a single unit". Ниже приведен пример детекции логотипа Т-Банка с выбранным промптом.

<div align="center"><img src="readme_pictures\OWLv2_TBank_find.JPG" width="50%" /></div>

Для проверки качества этой разметки (и не только этой разметки, но и окончательного варинта разметки и точности обучаемых моделей в целом) был собран валидационный набор данных, состоящий из изображений, на которых изображены:
1. логотипы Т-Банка в разных вариациях (четкая проекция в СМИ и бытовые фото);
2. логотипы других компаний, в частности Роснефти и Райффайзен-банка, т.к. эти логотипы `OWLv2` достаточно часто путал с логотипом Т-Банка.

:heavy_exclamation_mark: Этот набор данных можно найти в директории `my_val_dataseet_labeled` этого репозитория.

Ниже представлены метрики качества разметки на данном этапе, тестирование происходило на описанном выше датасете.

<img src="readme_pictures\Metrics_OWLv2.JPG" width="70%" />

Мы можем заметить, что при очень низкой точности (Precision) и других показателях мы имеем высокий показатель полноты (Recall). Это говорит о том, что наша модель находит большое количество логотипов Т-Банка, однако имеет множество ложных срабатываний. Проще говоря, модель способна находить логотипы Т-Банка на изображении, но помимо этого как логотип Т-Банка она выделяет еще и другие объекты (очень часто путает с логотипами Роснефти и Райффайзен-банка, поэтому в валидационный датасет были добавлены подобные изображения). Зафиксировав эту мысль, перейдем ко второму этапу работы с разметкой.

Замечание 1. Во время первичной разметки данных при помощи OWLv2, предоставленных организаторами, были обнаружены два интересных файла: `137bfd4864f4b4267fcd40e42c9d781e.jpg` и `2775f338c469b19c338c4e0ea410271c.jpg`, представляющих с собой белый и черный пиксели соответственно. Возможно, это артефакты, которые произошли при выкачивании данных. Однако я посчитала необходимым отметить этот факт на случай, если данный датасет будет применяться где-либо еще.

Замечание 2. Во время первичной разметки исходного датасета при помощи OWLv2 применялся алгоритм постобработки Non-Maximum Suppression (NMS), используемый для фильтрации избыточных ограничивающих рамок обнаруженных объектов (порог IoU = 0.5).

### 2-ой этап. Обучение и использование классификатора.

В связи с описанным выше наблюдением, возникла идея - обучить классификатор распознавать, что именно изображено на участках изображений, выделенных `OWLv2`. Если на выделенном участке не изображен логотип Т-Банка, то мы не будем принимать во внимание и добавлять в обучающий набор этот bounding box. Если опознано изображение логотипа Т-Банка, то будем добавлять соответсвующий bounding box в обучающую выборку.

За основу классификатора был взят `resnet18`, предобученный на ImageNet. Для обучения классификатора был собран датасет на 6 классов:

1. логотип Т-Банка (1_TBank_only_logo_224);
2. логотип Роснефти (2_Rosneft_224);
3. логотип Райффайзен-Банка (3_Raiffazen_224);
4. другие логотипы (4_Other_224). 
5. логотип Тинькофф-банка (5_Tinkoff_224);
6. буква "Т" с логотипа Т-Банка (1_OnlyT_224);

Классы выбирались на основе тех типов логотипов, которые модель чаще всего принимала за логотип Т-Банка. Логотипы в собранной обучабщей выборке выглядят следующим образом: 
<div align="center"><img src="readme_pictures\Only_TBank_logo.jpg" width="10%" /></div>

Такой обрезанный формат необходим, так как классификатор будет распознавать содержимое внутри bounding box.

Среди всех классов можно выделить пятый класс, содержащий изображения буквы "Т" с логотипа т-Банка. Он необходим, так как в первичной разметке модель иногда ошибочно выделяла как логотип целиком, так и букву Т на нем.

Метрики качества классификатора приведены ниже.
<div align="center"><img src="readme_pictures\classifier_metrics.jpg" width="100%" /></div>
Точность предсказаний обученного классификатора будем считать удовлетворительным.

Ниже приведены примеры, слева - как было до обработки классификатором, справа - как стало после обработки.
 
<p align="center">
  <img src="readme_pictures\With_T.jpg" width="40%"/>      
  <img src="readme_pictures\Without_T.jpg" width="40%"/> 
</p>

Еще пример. Слева - было, справа - стало. Можно заметить, что чип на карте больше не входит в обучающую выборку. 

<p align="center">
  <img src="readme_pictures\Card_before.jpg" width="40%"/>
  <img src="readme_pictures\Card_after.jpg" width="40%"/> 
</p>

На уже упомянутом валидационном датасете было проведено исследование точности вторичной разметки при помощи классификатора. Результаты приведены ниже.

<img src="readme_pictures\Metrics_classifier.JPG" width="70%" />

Можем заметить, что точность увеличилась, а число ложных срабатываний значительно уменьшилось. Будем считать полученные результаты удовлетворительными, теперь мы имеем размеченный набор исходных данных, предоставленных организаторами.

### Обучение и тестирование YOLOv8.

На полученном датасете была обучена модель `YOLOv8` (нано версия, далее будет обозначаться `YOLOv8n`). Ниже приведены метрики качества на размеченном вручную датасете, который мы уже использовали дважды для оценки качества первичной (после `OWLv2`) и вторичной (после `OWLv2` и классификатора) разметок. Результаты приведены ниже.

<div align="center"><img src="readme_pictures\Without_augs.jpg" width="70%" /></div>

Замечание. Забегая вперед, отмечу, что именно эта модель была интегрирована в REST-API сервис. Для того, чтобы воспроизвести результаты тестирования, в приложение был добавлен эндпоинт `/validate`. Он возвращает метрики по результатам тестирования на размеченном вручную валидационном датасете. Либо же можно воспользоваться скриптом `test_on_my_val_dataset_labeled.py` из директории `test_model` для получения метрик и скриптом `draw_on_my_val_dataset_labeled.py` для отрисовки результатов предсказаний (они будут сохранены в директории `draw_my_val_dataset_labeled`). 

Также было произведено еще одно обучение с аугментациями перспективы и геометрии, ниже приведены его метрики качества. 

<div align="center"><img src="readme_pictures\With_augs.jpg" width="70%" /></div>

В целом, ошутимой разницы нет, поэтому будем считать модели эквивалентными. 

Возвращаясь к началу, вспомним, что мы хотели сравнить нашу модель с моделью, обученной на синтетических данных (использовалась та же `YOLOv8n`). Синтетические данные были получены следующим образом: различные ваирнаты логотипа Т-Банка накладывались на фоновые изображения или на шум. Размер синтетического датасета сопоставим с размером автоматически размеченного датасета. Ниже приведены метрики качecтва для модели `YOLOv8n`, обученной на синтетических данных.

<div align="center"><img src="readme_pictures\Synth.jpg" width="70%" /></div>

Легко заметить, что последняя модель явно проигрывает первым двум, что доказывает правильность выбранного подхода с автоматической разметкой. Из всех описанных выше моделей выберем первую (обученную на данных без аугментаций)  для последующего ее интегрирования в REST-API сервис. 

Замечание. В самом верху этого README.md файла представлен пример детекции логотипа Т-Банка выбранной моделью. :blush:

## Архитектура REST-API сервиса

В соответствии со спецификацией, предоставленной в техническом задании, был разработан REST-API сервис со следующими эндпоинтами (скриншоты приведены из документации Swagger, доступной по адресу `http://127.0.0.1:8000/docs`):
1. `/detect` (метод POST). Получает на вход изображение и возвращает координаты найденных логотипов.
<div align="center"><img src="readme_pictures\detect.JPG" width="80%" /></div>

2. `/validate` (метод POST). Возвращает метрики по результатам тестирования на размеченном вручную валидационном датасете, который мы несколько раз уже упоминали.
<div align="center"><img src="readme_pictures\validate.JPG" width="80%" /></div>

3. `/health` (метод GET). Возвращает состояние сервиса.
<div align="center"><img src="readme_pictures\health.JPG" width="80%" /></div>

4. `/device-info` (метод GET). Возвращает информацию о вычислителе, на котором работает модель.
<div align="center"><img src="readme_pictures\device_info.JPG" width="80%" /></div>

5. `/supported_formats` (метод GET). Возвращает поддерживаемые форматы изображений (эти форматы соответствуют техническому заданию).
<div align="center"><img src="readme_pictures\supported_formats.JPG" width="80%" /></div>

6. `/validation_config` (метод GET). Возвращает информацию о доступности валидационного датасета к работе.
<div align="center"><img src="readme_pictures\validation_config.JPG" width="80%" /></div>

## Выводы
Предложенный метод хорошо распознает логотипы Т-Банка на изображениях с малыми геометрическими искажениями. 

Однако у решения есть следующая проблема - оно плохо распознает логотипы, искаженные в пространстве. Для решения этой проблемы следует поэкспериментировать с моделями семейства `YOLO` с бОльшим количеством обучаемых весов, а так же уделить внимание добавлению в обучающую выборку изображений с логотипом в перспективе.

Также в процесс подготовки данных производился на серверах Google Colab, и предоставляемых ресурсов не хватило для разметки всего датасета (размечено 2/3 части исходных данных). Доступ к видеокарте на локальной машине, к сожалению, у меня появился только в последних числах, отведенных на выполнение тестового задания, и времени на произведение разметки оставшихся данных не хватило. 
